{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T09:33:21.910688Z",
     "start_time": "2024-11-20T09:30:44.160073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Load SpaCy model and dataset\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text_data = load_dataset('wikitext', 'wikitext-2-raw-v1', split=\"train\")\n",
    "\n",
    "# Initialize counts\n",
    "unigram_counts = defaultdict(int)\n",
    "bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "total_unigrams = 0\n",
    "\n",
    "# Train unigram and bigram models\n",
    "for doc in text_data:\n",
    "    # Process each line as a separate document\n",
    "    processed_doc = nlp(doc['text'])\n",
    "    lemmas = ['START']  # Begin each document with START for bigram\n",
    "\n",
    "    # Collect lemmas, filtering out non-alpha tokens\n",
    "    for token in processed_doc:\n",
    "        if token.is_alpha:\n",
    "            lemma = token.lemma_.lower()\n",
    "            lemmas.append(lemma)\n",
    "            unigram_counts[lemma] += 1\n",
    "            total_unigrams += 1\n",
    "\n",
    "    # Count bigrams in the document\n",
    "    for i in range(1, len(lemmas)):\n",
    "        bigram_counts[lemmas[i - 1]][lemmas[i]] += 1\n",
    "\n",
    "# Filter out START from unigram_counts to avoid zero division\n",
    "unigram_counts = {k: v for k, v in unigram_counts.items() if v > 0}\n",
    "\n",
    "# Convert counts to log probabilities, avoiding zero division errors\n",
    "unigram_probs = {word: math.log(count / total_unigrams) for word, count in unigram_counts.items()}\n",
    "bigram_probs = {\n",
    "    w1: {w2: math.log(count / unigram_counts[w1]) for w2, count in following.items() if unigram_counts[w1] > 0}\n",
    "    for w1, following in bigram_counts.items()\n",
    "}\n",
    "\n"
   ],
   "id": "6ddbb2f35cdcae7a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomershav/NLP/Ex1/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/tomershav/NLP/Ex1/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'START'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 38\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Convert counts to log probabilities, avoiding zero division errors\u001B[39;00m\n\u001B[1;32m     37\u001B[0m unigram_probs \u001B[38;5;241m=\u001B[39m {word: math\u001B[38;5;241m.\u001B[39mlog(count \u001B[38;5;241m/\u001B[39m total_unigrams) \u001B[38;5;28;01mfor\u001B[39;00m word, count \u001B[38;5;129;01min\u001B[39;00m unigram_counts\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m---> 38\u001B[0m bigram_probs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     39\u001B[0m     w1: {w2: math\u001B[38;5;241m.\u001B[39mlog(count \u001B[38;5;241m/\u001B[39m unigram_counts[w1]) \u001B[38;5;28;01mfor\u001B[39;00m w2, count \u001B[38;5;129;01min\u001B[39;00m following\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m unigram_counts[w1] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m}\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m w1, following \u001B[38;5;129;01min\u001B[39;00m bigram_counts\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m     41\u001B[0m }\n",
      "Cell \u001B[0;32mIn[1], line 39\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Convert counts to log probabilities, avoiding zero division errors\u001B[39;00m\n\u001B[1;32m     37\u001B[0m unigram_probs \u001B[38;5;241m=\u001B[39m {word: math\u001B[38;5;241m.\u001B[39mlog(count \u001B[38;5;241m/\u001B[39m total_unigrams) \u001B[38;5;28;01mfor\u001B[39;00m word, count \u001B[38;5;129;01min\u001B[39;00m unigram_counts\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m     38\u001B[0m bigram_probs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m---> 39\u001B[0m     w1: {w2: math\u001B[38;5;241m.\u001B[39mlog(count \u001B[38;5;241m/\u001B[39m unigram_counts[w1]) \u001B[38;5;28;01mfor\u001B[39;00m w2, count \u001B[38;5;129;01min\u001B[39;00m following\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m unigram_counts[w1] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m}\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m w1, following \u001B[38;5;129;01min\u001B[39;00m bigram_counts\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m     41\u001B[0m }\n",
      "Cell \u001B[0;32mIn[1], line 39\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Convert counts to log probabilities, avoiding zero division errors\u001B[39;00m\n\u001B[1;32m     37\u001B[0m unigram_probs \u001B[38;5;241m=\u001B[39m {word: math\u001B[38;5;241m.\u001B[39mlog(count \u001B[38;5;241m/\u001B[39m total_unigrams) \u001B[38;5;28;01mfor\u001B[39;00m word, count \u001B[38;5;129;01min\u001B[39;00m unigram_counts\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m     38\u001B[0m bigram_probs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m---> 39\u001B[0m     w1: {w2: math\u001B[38;5;241m.\u001B[39mlog(count \u001B[38;5;241m/\u001B[39m unigram_counts[w1]) \u001B[38;5;28;01mfor\u001B[39;00m w2, count \u001B[38;5;129;01min\u001B[39;00m following\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m \u001B[43munigram_counts\u001B[49m\u001B[43m[\u001B[49m\u001B[43mw1\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m}\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m w1, following \u001B[38;5;129;01min\u001B[39;00m bigram_counts\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m     41\u001B[0m }\n",
      "\u001B[0;31mKeyError\u001B[0m: 'START'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------- Another Question ----------------\n",
    "# Using the bigram model to continue the sentence \"I have a house in ...\" with the most probable word\n",
    "\n",
    "# Initial prompt\n",
    "prompt = [\"i\", \"have\", \"a\", \"house\", \"in\"]\n",
    "\n",
    "# Convert the prompt to lemmas using SpaCy\n",
    "processed_prompt = nlp(\" \".join(prompt))\n",
    "lemmas_prompt = [token.lemma_.lower() for token in processed_prompt if token.is_alpha]\n",
    "\n",
    "# Get the last word of the prompt to find the most probable next word\n",
    "last_word = lemmas_prompt[-1]\n",
    "\n",
    "# Find the most probable next word based on the bigram model\n",
    "if last_word in bigram_probs:\n",
    "    next_word = max(bigram_probs[last_word], key=bigram_probs[last_word].get)\n",
    "    print(f\"The most probable next word after '{last_word}' is '{next_word}'\")\n",
    "else:\n",
    "    print(f\"No bigram data available for the word '{last_word}'.\")\n"
   ],
   "id": "fe35b59691ece857"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
