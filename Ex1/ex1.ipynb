{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-20T09:30:44.160073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Load SpaCy model and dataset\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text_data = load_dataset('wikitext', 'wikitext-2-raw-v1', split=\"train\")\n",
    "\n",
    "# Initialize counts\n",
    "unigram_counts = defaultdict(int)\n",
    "bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "total_unigrams = 0\n",
    "\n",
    "# Train unigram and bigram models\n",
    "for doc in text_data:\n",
    "    # Process each line as a separate document\n",
    "    processed_doc = nlp(doc['text'])\n",
    "    lemmas = ['START']  # Begin each document with START for bigram\n",
    "\n",
    "    # Collect lemmas, filtering out non-alpha tokens\n",
    "    for token in processed_doc:\n",
    "        if token.is_alpha:\n",
    "            lemma = token.lemma_.lower()\n",
    "            lemmas.append(lemma)\n",
    "            unigram_counts[lemma] += 1\n",
    "            total_unigrams += 1\n",
    "\n",
    "    # Count bigrams in the document\n",
    "    for i in range(1, len(lemmas)):\n",
    "        bigram_counts[lemmas[i - 1]][lemmas[i]] += 1\n",
    "\n",
    "# Filter out START from unigram_counts to avoid zero division\n",
    "unigram_counts = {k: v for k, v in unigram_counts.items() if v > 0}\n",
    "\n",
    "# Convert counts to log probabilities, avoiding zero division errors\n",
    "unigram_probs = {word: math.log(count / total_unigrams) for word, count in unigram_counts.items()}\n",
    "bigram_probs = {\n",
    "    w1: {w2: math.log(count / unigram_counts[w1]) for w2, count in following.items() if unigram_counts[w1] > 0}\n",
    "    for w1, following in bigram_counts.items()\n",
    "}\n",
    "\n"
   ],
   "id": "6ddbb2f35cdcae7a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomershav/NLP/Ex1/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/tomershav/NLP/Ex1/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------- Another Question ----------------\n",
    "# Using the bigram model to continue the sentence \"I have a house in ...\" with the most probable word\n",
    "\n",
    "# Initial prompt\n",
    "prompt = [\"i\", \"have\", \"a\", \"house\", \"in\"]\n",
    "\n",
    "# Convert the prompt to lemmas using SpaCy\n",
    "processed_prompt = nlp(\" \".join(prompt))\n",
    "lemmas_prompt = [token.lemma_.lower() for token in processed_prompt if token.is_alpha]\n",
    "\n",
    "# Get the last word of the prompt to find the most probable next word\n",
    "last_word = lemmas_prompt[-1]\n",
    "\n",
    "# Find the most probable next word based on the bigram model\n",
    "if last_word in bigram_probs:\n",
    "    next_word = max(bigram_probs[last_word], key=bigram_probs[last_word].get)\n",
    "    print(f\"The most probable next word after '{last_word}' is '{next_word}'\")\n",
    "else:\n",
    "    print(f\"No bigram data available for the word '{last_word}'.\")\n"
   ],
   "id": "fe35b59691ece857"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
