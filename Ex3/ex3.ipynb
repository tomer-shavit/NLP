{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tomer\\uni\\nlp\\ex3\\venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\tomer\\uni\\nlp\\ex3\\venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\tomer\\uni\\nlp\\ex3\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tomer\\uni\\nlp\\ex3\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tomer\\uni\\nlp\\ex3\\venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tomer\\uni\\nlp\\ex3\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "     ---------------------------------------- 11.1/11.1 MB 6.4 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.14.1-cp310-cp310-win_amd64.whl (44.8 MB)\n",
      "Collecting numpy>=1.19.5\n",
      "  Downloading numpy-2.2.0-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "     ---------------------------------------- 12.9/12.9 MB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tomer\\uni\\nlp\\ex3\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Installing collected packages: threadpoolctl, numpy, scipy, scikit-learn\n",
      "Successfully installed numpy-2.2.0 scikit-learn-1.6.0 scipy-1.14.1 threadpoolctl-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\tomer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download the Brown corpus\n",
    "nltk.download('brown')\n",
    "\n",
    "# Load the tagged sentences from the \"news\" category\n",
    "tagged_sents = brown.tagged_sents(categories='news')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in training set: 4160\n",
      "Number of sentences in testing set: 463\n",
      "Dataset preparation complete.\n"
     ]
    }
   ],
   "source": [
    "def simplify_tag(tag):\n",
    "    \"\"\"\n",
    "    Simplifies a complex tag by taking only the prefix before the first occurrence\n",
    "    of '+' or '-'.\n",
    "    \"\"\"\n",
    "    if '+' in tag:\n",
    "        return tag.split('+')[0]\n",
    "    if '-' in tag:\n",
    "        return tag.split('-')[0]\n",
    "    return tag\n",
    "\n",
    "# Simplify tags in the corpus\n",
    "simplified_tagged_sents = [\n",
    "    [(word, simplify_tag(tag)) for word, tag in sentence]\n",
    "    for sentence in tagged_sents\n",
    "]\n",
    "\n",
    "# Split the dataset into training (90%) and testing (10%)\n",
    "train_data, test_data = train_test_split(\n",
    "    simplified_tagged_sents, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Display dataset sizes\n",
    "print(f\"Number of sentences in training set: {len(train_data)}\")\n",
    "print(f\"Number of sentences in testing set: {len(test_data)}\")\n",
    "\n",
    "# Save the training and testing datasets for further usage\n",
    "with open(\"train_data.pkl\", \"wb\") as train_file, open(\"test_data.pkl\", \"wb\") as test_file:\n",
    "    import pickle\n",
    "    pickle.dump(train_data, train_file)\n",
    "    pickle.dump(test_data, test_file)\n",
    "\n",
    "print(\"Dataset preparation complete.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
