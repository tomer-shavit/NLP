{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from nltk.corpus import brown\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "split_point = math.floor(len(tagged_sents) * 0.9)\n",
    "\n",
    "train_sents = tagged_sents[:split_point]\n",
    "test_sents = tagged_sents[split_point:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T13:16:29.207335Z",
     "start_time": "2024-12-18T13:16:29.078141Z"
    }
   },
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Most Likely Tag Baseline Results ===\n",
      "Error Rate for Known Words: 0.0832\n",
      "Error Rate for Unknown Words: 0.7897\n",
      "Total Error Rate: 0.1639\n"
     ]
    }
   ],
   "source": [
    "word_tag_counts = defaultdict(lambda: defaultdict(int))\n",
    "for sent in train_sents:\n",
    "    for word, tag in sent:\n",
    "        word_tag_counts[word][tag] += 1\n",
    "\n",
    "word_most_freq_tag = {}\n",
    "\n",
    "for word, tag_counts in word_tag_counts.items():\n",
    "    most_freq_tag = max(tag_counts.items(), key=lambda item: item[1])[0]\n",
    "    word_most_freq_tag[word] = most_freq_tag\n",
    "\n",
    "known_words = set(word_most_freq_tag.keys())\n",
    "\n",
    "# Part (b)ii: Evaluate the baseline on the test set\n",
    "total_known = 0\n",
    "errors_known = 0\n",
    "\n",
    "total_unknown = 0\n",
    "errors_unknown = 0\n",
    "\n",
    "total = 0\n",
    "errors = 0\n",
    "\n",
    "for sent in test_sents:\n",
    "    for word, true_tag in sent:\n",
    "        total += 1\n",
    "        if word in known_words:\n",
    "            predicted_tag = word_most_freq_tag[word]\n",
    "            total_known += 1\n",
    "            if predicted_tag != true_tag:\n",
    "                errors_known += 1\n",
    "        else:\n",
    "            predicted_tag = 'NN'\n",
    "            total_unknown += 1\n",
    "            if predicted_tag != true_tag:\n",
    "                errors_unknown += 1\n",
    "\n",
    "error_rate_known = errors_known / total_known if total_known > 0 else 0\n",
    "error_rate_unknown = errors_unknown / total_unknown if total_unknown > 0 else 0\n",
    "total_error_rate = (errors_known + errors_unknown) / total if total > 0 else 0\n",
    "\n",
    "print(\"=== Most Likely Tag Baseline Results ===\")\n",
    "print(f\"Error Rate for Known Words: {error_rate_known:.4f}\")\n",
    "print(f\"Error Rate for Unknown Words: {error_rate_unknown:.4f}\")\n",
    "print(f\"Total Error Rate: {total_error_rate:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T13:16:32.238721Z",
     "start_time": "2024-12-18T13:16:32.201807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "split_point = math.floor(len(tagged_sents) * 0.9)\n",
    "\n",
    "train_sents = tagged_sents[:split_point]\n",
    "test_sents = tagged_sents[split_point:]\n",
    "\n",
    "# Part (c)i: Training phase\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "START = \"<s>\"\n",
    "\n",
    "for sent in train_sents:\n",
    "    prev_tag = START\n",
    "    tag_counts[prev_tag] += 1\n",
    "    for word, tag in sent:\n",
    "        transition_counts[prev_tag][tag] += 1\n",
    "        emission_counts[tag][word] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        prev_tag = tag\n",
    "\n",
    "transition_probs = defaultdict(dict)\n",
    "for prev_tag in transition_counts:\n",
    "    total = sum(transition_counts[prev_tag].values())\n",
    "    for tag in transition_counts[prev_tag]:\n",
    "        transition_probs[prev_tag][tag] = transition_counts[prev_tag][tag] / total\n",
    "\n",
    "emission_probs = defaultdict(dict)\n",
    "for tag in emission_counts:\n",
    "    total = sum(emission_counts[tag].values())\n",
    "    for word in emission_counts[tag]:\n",
    "        emission_probs[tag][word] = emission_counts[tag][word] / total\n",
    "\n",
    "\n",
    "# Part (c)ii: Viterbi algorithm\n",
    "def viterbi(sentence, transition_probs, emission_probs, tags, start_tag=START, unknown_tag='NN'):\n",
    "    MIN_PROB = 1e-6\n",
    "    trellis = [{}]\n",
    "    paths = {}\n",
    "\n",
    "    first_word = sentence[0]\n",
    "    for tag in tags:\n",
    "        transition_probability = transition_probs[start_tag].get(tag, MIN_PROB)\n",
    "        emission_probability = emission_probs[tag].get(first_word, emission_probs[tag].get(unknown_tag, MIN_PROB))\n",
    "        trellis[0][tag] = transition_probability * emission_probability\n",
    "        paths[tag] = [tag]\n",
    "\n",
    "    for position in range(1, len(sentence)):\n",
    "        trellis.append({})\n",
    "        new_paths = {}\n",
    "        current_word = sentence[position]\n",
    "        for current_tag in tags:\n",
    "            emission_probability = emission_probs[current_tag].get(\n",
    "                current_word,\n",
    "                emission_probs[current_tag].get(unknown_tag, MIN_PROB)\n",
    "            )\n",
    "            max_probability = 0\n",
    "            best_previous_tag = None\n",
    "            for previous_tag in tags:\n",
    "                transition_probability = transition_probs[previous_tag].get(current_tag, MIN_PROB)\n",
    "                probability = trellis[position - 1][previous_tag] * transition_probability * emission_probability\n",
    "                if probability > max_probability:\n",
    "                    max_probability = probability\n",
    "                    best_previous_tag = previous_tag\n",
    "            trellis[position][current_tag] = max_probability\n",
    "            new_paths[current_tag] = paths[best_previous_tag] + [current_tag]\n",
    "        paths = new_paths\n",
    "\n",
    "    best_final_tag = max(trellis[-1], key=trellis[-1].get)\n",
    "    return paths[best_final_tag]\n",
    "\n",
    "\n",
    "# Part (c)iii: Running on test set and computing error rates\n",
    "\n",
    "known_words = set()\n",
    "for tag in emission_counts:\n",
    "    for word in emission_counts[tag]:\n",
    "        known_words.add(word)\n",
    "\n",
    "tags = list(tag_counts.keys())\n",
    "unknown_tag = 'NN'  # Arbitrary tag for unknown words\n",
    "\n",
    "total = 0\n",
    "errors = 0\n",
    "\n",
    "total_known = 0\n",
    "errors_known = 0\n",
    "\n",
    "total_unknown = 0\n",
    "errors_unknown = 0\n",
    "\n",
    "for sent in test_sents:\n",
    "    words, true_tags = zip(*sent)\n",
    "    predicted_tags = viterbi(words, transition_probs, emission_probs, tags, unknown_tag=unknown_tag)\n",
    "    for word, pred, true in zip(words, predicted_tags, true_tags):\n",
    "        total += 1\n",
    "        if pred != true:\n",
    "            errors += 1\n",
    "        if word in known_words:\n",
    "            total_known += 1\n",
    "            if pred != true:\n",
    "                errors_known += 1\n",
    "        else:\n",
    "            total_unknown += 1\n",
    "            if pred != true:\n",
    "                errors_unknown += 1\n",
    "\n",
    "error_rate_known = errors_known / total_known if total_known > 0 else 0\n",
    "error_rate_unknown = errors_unknown / total_unknown if total_unknown > 0 else 0\n",
    "total_error_rate = errors / total if total > 0 else 0\n",
    "\n",
    "print(f\"Error Rate for Known Words: {error_rate_known:.4f}\")\n",
    "print(f\"Error Rate for Unknown Words: {error_rate_unknown:.4f}\")\n",
    "print(f\"Total Error Rate: {total_error_rate:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate for Known Words: 0.0479\n",
      "Error Rate for Unknown Words: 0.7190\n",
      "Total Error Rate: 0.1246\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T13:36:19.063752Z",
     "start_time": "2024-12-18T13:34:58.372477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "V = len(known_words) + 1  # +1 for handling unknown words as '<UNK>'\n",
    "emission_probs_smooth = defaultdict(dict)\n",
    "\n",
    "for tag in emission_counts:\n",
    "    denominator = tag_counts[tag] + V\n",
    "    for word in emission_counts[tag]:\n",
    "        emission_probs_smooth[tag][word] = (emission_counts[tag][word] + 1) / denominator\n",
    "    emission_probs_smooth[tag]['<UNK>'] = 1 / denominator\n",
    "\n",
    "def replace_unknowns(sentence, known_words):\n",
    "    return [word if word in known_words else '<UNK>' for word in sentence]\n",
    "\n",
    "total = 0\n",
    "errors = 0\n",
    "\n",
    "total_known = 0\n",
    "errors_known = 0\n",
    "\n",
    "total_unknown = 0\n",
    "errors_unknown = 0\n",
    "\n",
    "for sent in test_sents:\n",
    "    words, true_tags = zip(*sent)\n",
    "    processed_words = replace_unknowns(words, known_words)\n",
    "    predicted_tags = viterbi(processed_words, transition_probs, emission_probs_smooth, tags, unknown_tag='<UNK>')\n",
    "    for word, pred, true in zip(words, predicted_tags, true_tags):\n",
    "        total += 1\n",
    "        if pred != true:\n",
    "            errors += 1\n",
    "        if word in known_words:\n",
    "            total_known += 1\n",
    "            if pred != true:\n",
    "                errors_known += 1\n",
    "        else:\n",
    "            total_unknown += 1\n",
    "            if pred != true:\n",
    "                errors_unknown += 1\n",
    "\n",
    "error_rate_known = errors_known / total_known if total_known > 0 else 0\n",
    "error_rate_unknown = errors_unknown / total_unknown if total_unknown > 0 else 0\n",
    "total_error_rate = errors / total if total > 0 else 0\n",
    "\n",
    "print(\"=== Add-One Smoothing Results ===\")\n",
    "print(f\"Error Rate for Known Words: {error_rate_known:.4f}\")\n",
    "print(f\"Error Rate for Unknown Words: {error_rate_unknown:.4f}\")\n",
    "print(f\"Total Error Rate: {total_error_rate:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Add-One Smoothing Results ===\n",
      "Error Rate for Known Words: 0.1788\n",
      "Error Rate for Unknown Words: 0.7522\n",
      "Total Error Rate: 0.2443\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pseudo-Words with MLE Results ===\n",
      "Error Rate for Known Words: 0.1858\n",
      "Error Rate for Unknown Words: 0.0000\n",
      "Total Error Rate: 0.1858\n"
     ]
    }
   ],
   "source": [
    "FREQUENCY_THRESHOLD = 5\n",
    "\n",
    "def create_pseudo_word(word, frequency):\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return '_NUM'\n",
    "    elif word.isupper():\n",
    "        return '_ALLCAPS'\n",
    "    elif word[0].isupper():\n",
    "        return '_CAPITAL'\n",
    "    elif word.endswith('ing'):\n",
    "        return '_ING'\n",
    "    elif word.endswith('ed'):\n",
    "        return '_ED'\n",
    "    elif word.endswith('s'):\n",
    "        return '_S'\n",
    "    elif word.endswith('ly'):\n",
    "        return '_LY'\n",
    "    elif frequency < FREQUENCY_THRESHOLD:\n",
    "        return '_UNK'\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "for sent in train_sents:\n",
    "    for word, tag in sent:\n",
    "        word_freq[word] += 1\n",
    "\n",
    "processed_train_sents_pseudo = []\n",
    "for sent in train_sents:\n",
    "    new_sent = []\n",
    "    for word, tag in sent:\n",
    "        pseudo_word = create_pseudo_word(word, word_freq[word])\n",
    "        new_sent.append((pseudo_word, tag))\n",
    "    processed_train_sents_pseudo.append(new_sent)\n",
    "\n",
    "processed_test_sents_pseudo = []\n",
    "for sent in test_sents:\n",
    "    new_sent = []\n",
    "    for word, tag in sent:\n",
    "        pseudo_word = create_pseudo_word(word, word_freq.get(word, 0))\n",
    "        new_sent.append((pseudo_word, tag))\n",
    "    processed_test_sents_pseudo.append(new_sent)\n",
    "\n",
    "transition_counts_pseudo = defaultdict(lambda: defaultdict(int))\n",
    "emission_counts_pseudo = defaultdict(lambda: defaultdict(int))\n",
    "tag_counts_pseudo = defaultdict(int)\n",
    "\n",
    "for sent in processed_train_sents_pseudo:\n",
    "    prev_tag = START\n",
    "    tag_counts_pseudo[prev_tag] += 1\n",
    "    for word, tag in sent:\n",
    "        transition_counts_pseudo[prev_tag][tag] += 1\n",
    "        emission_counts_pseudo[tag][word] += 1\n",
    "        tag_counts_pseudo[tag] += 1\n",
    "        prev_tag = tag\n",
    "\n",
    "transition_probs_pseudo = defaultdict(dict)\n",
    "for prev_tag in transition_counts_pseudo:\n",
    "    total = sum(transition_counts_pseudo[prev_tag].values())\n",
    "    for tag in transition_counts_pseudo[prev_tag]:\n",
    "        transition_probs_pseudo[prev_tag][tag] = transition_counts_pseudo[prev_tag][tag] / total\n",
    "\n",
    "emission_probs_pseudo = defaultdict(dict)\n",
    "for tag in emission_counts_pseudo:\n",
    "    total = sum(emission_counts_pseudo[tag].values())\n",
    "    for word in emission_counts_pseudo[tag]:\n",
    "        emission_probs_pseudo[tag][word] = emission_counts_pseudo[tag][word] / total\n",
    "\n",
    "known_pseudo_words = set()\n",
    "for tag in emission_counts_pseudo:\n",
    "    for word in emission_counts_pseudo[tag]:\n",
    "        known_pseudo_words.add(word)\n",
    "\n",
    "tags_pseudo = list(tag_counts_pseudo.keys())\n",
    "\n",
    "unknown_tag_pseudo = 'NN'\n",
    "\n",
    "total_pseudo = 0\n",
    "errors_pseudo = 0\n",
    "\n",
    "total_known_pseudo = 0\n",
    "errors_known_pseudo = 0\n",
    "\n",
    "total_unknown_pseudo = 0\n",
    "errors_unknown_pseudo = 0\n",
    "\n",
    "for sent in processed_test_sents_pseudo:\n",
    "    words, true_tags = zip(*sent)\n",
    "    predicted_tags = viterbi(words, transition_probs_pseudo, emission_probs_pseudo, tags_pseudo, unknown_tag=unknown_tag_pseudo)\n",
    "    for word, pred, true in zip(words, predicted_tags, true_tags):\n",
    "        total_pseudo += 1\n",
    "        if pred != true:\n",
    "            errors_pseudo += 1\n",
    "        if word in known_pseudo_words:\n",
    "            total_known_pseudo += 1\n",
    "            if pred != true:\n",
    "                errors_known_pseudo += 1\n",
    "        else:\n",
    "            total_unknown_pseudo += 1\n",
    "            if pred != true:\n",
    "                errors_unknown_pseudo += 1\n",
    "\n",
    "error_rate_known_pseudo = errors_known_pseudo / total_known_pseudo if total_known_pseudo > 0 else 0\n",
    "error_rate_unknown_pseudo = errors_unknown_pseudo / total_unknown_pseudo if total_unknown_pseudo > 0 else 0\n",
    "total_error_rate_pseudo = errors_pseudo / total_pseudo if total_pseudo > 0 else 1\n",
    "\n",
    "# Print the results\n",
    "print(\"=== Pseudo-Words with MLE Results ===\")\n",
    "print(f\"Error Rate for Known Words: {error_rate_known_pseudo:.4f}\")\n",
    "print(f\"Error Rate for Unknown Words: {error_rate_unknown_pseudo:.4f}\")\n",
    "print(f\"Total Error Rate: {total_error_rate_pseudo:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pseudo-Words with Add-One Smoothing Results ===\n",
      "Error Rate for Known Words: 0.1903\n",
      "Error Rate for Unknown Words: 0.0000\n",
      "Total Error Rate: 0.1903\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "         '  ''   (  (-HL   )  )-HL  *  *-HL  ,  ,-HL  ...  VBZ-HL  WDT  \\\n",
      "'        5   0   0     0   0     0  0     0  0     0  ...       0    0   \n",
      "''       0  50   0     0   0     0  0     0  0     0  ...       0    0   \n",
      "(        0   0  17     0   0     0  0     0  0     0  ...       0    0   \n",
      "(-HL     0   0   0     0   0     0  0     0  0     0  ...       0    0   \n",
      ")        0   0   0     0  13     0  0     0  3     0  ...       0    0   \n",
      "...     ..  ..  ..   ...  ..   ... ..   ... ..   ...  ...     ...  ...   \n",
      "WPS      0   0   0     0   0     0  0     0  0     0  ...       0    0   \n",
      "WPS+BEZ  0   0   0     0   0     0  0     0  0     0  ...       0    0   \n",
      "WQL      0   0   0     0   0     0  0     0  0     0  ...       0    0   \n",
      "WRB      0   0   0     0   0     0  0     0  0     0  ...       0    0   \n",
      "``       0   0   0     0   0     0  0     0  0     0  ...       0    0   \n",
      "\n",
      "         WDT+BEZ  WP$  WPO  WPS  WPS+BEZ  WQL  WRB  ``  \n",
      "'              0    0    0    0        0    0    0   0  \n",
      "''             0    0    0    0        0    0    0   0  \n",
      "(              0    0    0    0        0    0    0   0  \n",
      "(-HL           0    0    0    0        0    0    0   0  \n",
      ")              0    0    0    0        0    0    0   0  \n",
      "...          ...  ...  ...  ...      ...  ...  ...  ..  \n",
      "WPS            0    0    0   37        0    0    0   0  \n",
      "WPS+BEZ        0    0    0    0        0    0    0   0  \n",
      "WQL            0    0    0    0        0    0    0   0  \n",
      "WRB            0    0    0    0        0    0   27   0  \n",
      "``             0    0    0    0        0    0    0  58  \n",
      "\n",
      "[219 rows x 219 columns]\n",
      "\n",
      "=== Most Frequent Errors ===\n",
      "NP   NN-TL    163\n",
      "JJ   NN        84\n",
      "NN   JJ        62\n",
      "NP   JJ-TL     50\n",
      "     NP-TL     50\n",
      "     AT        38\n",
      "VBD  VBN       35\n",
      "NNS  NN        33\n",
      "VBN  VBD       33\n",
      "NNS  VBZ       32\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "unique_test_tags_pseudo = set(tag for sent in processed_test_sents_pseudo for _, tag in sent)\n",
    "unique_all_tags_pseudo = sorted(list(set(tags_pseudo).union(unique_test_tags_pseudo)))\n",
    "confusion_matrix = pd.DataFrame(0, index=unique_all_tags_pseudo, columns=unique_all_tags_pseudo)\n",
    "\n",
    "word_freq_pseudo = defaultdict(int)\n",
    "for sent in train_sents:\n",
    "    for word, tag in sent:\n",
    "        word_freq_pseudo[word] += 1\n",
    "\n",
    "processed_train_sents_pseudo = []\n",
    "for sent in train_sents:\n",
    "    new_sent = []\n",
    "    for word, tag in sent:\n",
    "        pseudo_word = create_pseudo_word(word, word_freq_pseudo[word])\n",
    "        new_sent.append((pseudo_word, tag))\n",
    "    processed_train_sents_pseudo.append(new_sent)\n",
    "\n",
    "processed_test_sents_pseudo = []\n",
    "for sent in test_sents:\n",
    "    new_sent = []\n",
    "    for word, tag in sent:\n",
    "        pseudo_word = create_pseudo_word(word, word_freq_pseudo.get(word, 0))\n",
    "        new_sent.append((pseudo_word, tag))\n",
    "    processed_test_sents_pseudo.append(new_sent)\n",
    "\n",
    "transition_counts_pseudo = defaultdict(lambda: defaultdict(int))\n",
    "emission_counts_pseudo = defaultdict(lambda: defaultdict(int))\n",
    "tag_counts_pseudo = defaultdict(int)\n",
    "\n",
    "for sent in processed_train_sents_pseudo:\n",
    "    prev_tag = START\n",
    "    tag_counts_pseudo[prev_tag] += 1\n",
    "    for word, tag in sent:\n",
    "        transition_counts_pseudo[prev_tag][tag] += 1\n",
    "        emission_counts_pseudo[tag][word] += 1\n",
    "        tag_counts_pseudo[tag] += 1\n",
    "        prev_tag = tag\n",
    "\n",
    "transition_probs_pseudo = defaultdict(dict)\n",
    "for prev_tag in transition_counts_pseudo:\n",
    "    total = sum(transition_counts_pseudo[prev_tag].values())\n",
    "    for tag in transition_counts_pseudo[prev_tag]:\n",
    "        transition_probs_pseudo[prev_tag][tag] = transition_counts_pseudo[prev_tag][tag] / total\n",
    "\n",
    "emission_probs_pseudo_smooth = defaultdict(dict)\n",
    "V_pseudo = len(set(word for tag in emission_counts_pseudo for word in emission_counts_pseudo[tag])) + 1  # +1 for '<UNK>'\n",
    "\n",
    "for tag in emission_counts_pseudo:\n",
    "    total = sum(emission_counts_pseudo[tag].values()) + V_pseudo  # Add-One Smoothing\n",
    "    for word in emission_counts_pseudo[tag]:\n",
    "        emission_probs_pseudo_smooth[tag][word] = (emission_counts_pseudo[tag][word] + 1) / total\n",
    "    # Probability for unknown words\n",
    "    emission_probs_pseudo_smooth[tag]['<UNK>'] = 1 / total\n",
    "\n",
    "known_pseudo_words = set()\n",
    "for tag in emission_counts_pseudo:\n",
    "    for word in emission_counts_pseudo[tag]:\n",
    "        known_pseudo_words.add(word)\n",
    "\n",
    "tags_pseudo = list(tag_counts_pseudo.keys())\n",
    "\n",
    "unknown_tag_pseudo = 'NN'\n",
    "\n",
    "total_pseudo_smooth = 0\n",
    "errors_pseudo_smooth = 0\n",
    "\n",
    "total_known_pseudo_smooth = 0\n",
    "errors_known_pseudo_smooth = 0\n",
    "\n",
    "total_unknown_pseudo_smooth = 0\n",
    "errors_unknown_pseudo_smooth = 0\n",
    "\n",
    "for sent in processed_test_sents_pseudo:\n",
    "    words, true_tags = zip(*sent)\n",
    "    processed_words = [word if word in known_pseudo_words else '<UNK>' for word in words]\n",
    "    predicted_tags = viterbi(processed_words, transition_probs_pseudo, emission_probs_pseudo_smooth, tags_pseudo, unknown_tag=unknown_tag_pseudo)\n",
    "    for word, pred, true in zip(words, predicted_tags, true_tags):\n",
    "        total_pseudo_smooth += 1\n",
    "        if pred != true:\n",
    "            errors_pseudo_smooth += 1\n",
    "        if word in known_pseudo_words:\n",
    "            total_known_pseudo_smooth += 1\n",
    "            if pred != true:\n",
    "                errors_known_pseudo_smooth += 1\n",
    "        else:\n",
    "            total_unknown_pseudo_smooth += 1\n",
    "            if pred != true:\n",
    "                errors_unknown_pseudo_smooth += 1\n",
    "        confusion_matrix.loc[true, pred] += 1\n",
    "\n",
    "error_rate_known_pseudo_smooth = errors_known_pseudo_smooth / total_known_pseudo_smooth if total_known_pseudo_smooth > 0 else 0\n",
    "error_rate_unknown_pseudo_smooth = errors_unknown_pseudo_smooth / total_unknown_pseudo_smooth if total_unknown_pseudo_smooth > 0 else 0\n",
    "total_error_rate_pseudo_smooth = errors_pseudo_smooth / total_pseudo_smooth if total_pseudo_smooth > 0 else 0\n",
    "\n",
    "print(\"=== Pseudo-Words with Add-One Smoothing Results ===\")\n",
    "print(f\"Error Rate for Known Words: {error_rate_known_pseudo_smooth:.4f}\")\n",
    "print(f\"Error Rate for Unknown Words: {error_rate_unknown_pseudo_smooth:.4f}\")\n",
    "print(f\"Total Error Rate: {total_error_rate_pseudo_smooth:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(confusion_matrix)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "=== Pseudo-Words with Add-One Smoothing Results ===\n",
    "Error Rate for Known Words: 0.1903\n",
    "Error Rate for Unknown Words: 0.0000\n",
    "Total Error Rate: 0.1903\n",
    "\n",
    "=== Confusion Matrix ===\n",
    "         '  ''   (  (-HL   )  )-HL  *  *-HL  ,  ,-HL  ...  VBZ-HL  WDT  \\\n",
    "'        5   0   0     0   0     0  0     0  0     0  ...       0    0\n",
    "''       0  50   0     0   0     0  0     0  0     0  ...       0    0\n",
    "(        0   0  17     0   0     0  0     0  0     0  ...       0    0\n",
    "(-HL     0   0   0     0   0     0  0     0  0     0  ...       0    0\n",
    ")        0   0   0     0  13     0  0     0  3     0  ...       0    0\n",
    "...     ..  ..  ..   ...  ..   ... ..   ... ..   ...  ...     ...  ...\n",
    "WPS      0   0   0     0   0     0  0     0  0     0  ...       0    0\n",
    "WPS+BEZ  0   0   0     0   0     0  0     0  0     0  ...       0    0\n",
    "WQL      0   0   0     0   0     0  0     0  0     0  ...       0    0\n",
    "WRB      0   0   0     0   0     0  0     0  0     0  ...       0    0\n",
    "``       0   0   0     0   0     0  0     0  0     0  ...       0    0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
